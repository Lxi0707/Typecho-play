#!/usr/bin/env python3
import asyncio
import aiohttp
import random
import time
import sys
from bs4 import BeautifulSoup
import argparse
import logging
from typing import List, Dict, Tuple, DefaultDict
from collections import defaultdict
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class TypechoCrawler:
    def __init__(self, base_url: str, tg_bot_token: str = None, tg_chat_id: str = None):
        self.base_url = base_url.rstrip('/')
        self.session = None
        self.tg_bot_token = tg_bot_token
        self.tg_chat_id = tg_chat_id
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': self.base_url,
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.article_urls = []
        self.visit_stats = defaultdict(lambda: {'success': 0, 'failed': 0})

    async def init_session(self):
        connector = aiohttp.TCPConnector(limit=10, force_close=True)
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=60)
        )

    async def close_session(self):
        if self.session:
            await self.session.close()

    async def fetch_url_with_retry(self, url: str, max_retries: int = 3) -> Tuple[bool, str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„URLè®¿é—®"""
        last_error = ""
        for attempt in range(max_retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return True, ""
                    last_error = f"HTTP status {response.status}"
                    if response.status == 429:  # Too Many Requests
                        wait_time = (attempt + 1) * 5
                        logger.warning(f"Rate limited, waiting {wait_time} seconds before retry...")
                        await asyncio.sleep(wait_time)
            except Exception as e:
                last_error = str(e)
            
            if attempt < max_retries - 1:
                wait_time = random.uniform(1.0, 3.0) * (attempt + 1)
                logger.warning(f"Attempt {attempt + 1} failed for {url}, retrying in {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
        
        return False, last_error

    def is_valid_article_url(self, href: str) -> bool:
        """éªŒè¯URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡ç« URL"""
        if not href.startswith(f"{self.base_url}/index.php/archives/"):
            return False
        if not re.search(r'/archives/\d+/', href):
            return False
        return True

    async def get_article_urls(self, max_pages: int = 10) -> List[str]:
        """è·å–æ‰€æœ‰æ–‡ç« URL"""
        urls = []
        seen_urls = set()
        
        try:
            for page in range(1, max_pages + 1):
                list_url = f"{self.base_url}/index.php/archives/page/{page}" if page > 1 else self.base_url
                
                logger.info(f"Fetching articles from page {page}: {list_url}")
                success, error = await self.fetch_url_with_retry(list_url)
                
                if not success:
                    logger.warning(f"Failed to fetch page {page}: {error}")
                    continue
                
                async with self.session.get(list_url) as response:
                    html = await response.text()
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # å¤šç§é€‰æ‹©å™¨ç¡®ä¿æ‰¾åˆ°æ‰€æœ‰æ–‡ç« é“¾æ¥
                link_selectors = [
                    '.post-title a', 
                    'h2.title a',
                    'article header h2 a',
                    'a[href*="/archives/"]'
                ]
                
                found_links = False
                for selector in link_selectors:
                    links = soup.select(selector)
                    if links:
                        found_links = True
                        for link in links:
                            href = link.get('href', '').strip()
                            if not href:
                                continue
                                
                            # è§„èŒƒåŒ–URL
                            if href.startswith('/'):
                                href = f"{self.base_url}{href}"
                            elif not href.startswith(('http://', 'https://')):
                                href = f"{self.base_url}/{href}"
                            
                            if self.is_valid_article_url(href) and href not in seen_urls:
                                seen_urls.add(href)
                                urls.append(href)
                                logger.debug(f"Found article: {href}")
                        
                        if found_links:
                            break
                
                if not found_links:
                    logger.warning(f"No article links found on page {page} with selectors")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page = soup.select_one('.next, .page-navigator a:contains("ä¸‹ä¸€é¡µ"), a[rel="next"]')
                if not next_page:
                    logger.info(f"No more pages found after page {page}")
                    break
                
                await asyncio.sleep(random.uniform(1.0, 3.0))
        
        except Exception as e:
            logger.error(f"Error fetching article URLs: {str(e)}")
        
        if not urls:
            logger.warning("No articles found, using default URLs")
            urls = [
                f"{self.base_url}/index.php/archives/13/",
                f"{self.base_url}/index.php/archives/5/",
                f"{self.base_url}/index.php/archives/1/"
            ]
        
        self.article_urls = urls
        return urls

    async def simulate_visits(self, total_visits: int = 100, max_concurrent: int = 5):
        """ç²¾ç¡®åˆ†é…è®¿é—®é‡çš„æ¨¡æ‹Ÿè®¿é—®æ–¹æ³•"""
        if not self.article_urls:
            logger.error("No articles to visit")
            return
        
        # è®¡ç®—æ¯ç¯‡æ–‡ç« çš„åŸºç¡€è®¿é—®æ¬¡æ•°å’Œå‰©ä½™æ¬¡æ•°
        base_visits = total_visits // len(self.article_urls)
        remaining_visits = total_visits % len(self.article_urls)
        
        # åˆ›å»ºè®¿é—®ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for i, url in enumerate(self.article_urls):
            visits = base_visits + (1 if i < remaining_visits else 0)
            for _ in range(visits):
                tasks.append(url)
        
        # éšæœºæ‰“ä¹±ä»»åŠ¡é¡ºåº
        random.shuffle(tasks)
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def visit_with_semaphore(url):
            async with semaphore:
                # éšæœºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(1.0, 3.0))
                
                success, error = await self.fetch_url_with_retry(url)
                if success:
                    self.visit_stats[url]['success'] += 1
                else:
                    self.visit_stats[url]['failed'] += 1
                    logger.warning(f"Failed to visit {url}: {error}")
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info(f"Starting {len(tasks)} visits ({total_visits} requested)...")
        start_time = time.time()
        await asyncio.gather(*[visit_with_semaphore(url) for url in tasks])
        elapsed = time.time() - start_time
        logger.info(f"Completed {len(tasks)} visits in {elapsed:.1f} seconds")

    async def send_telegram_notification(self, message: str):
        """å‘é€Telegramé€šçŸ¥"""
        if not self.tg_bot_token or not self.tg_chat_id:
            logger.warning("Telegram bot token or chat ID not provided, skipping notification")
            return
        
        try:
            url = f"https://api.telegram.org/bot{self.tg_bot_token}/sendMessage"
            data = {
                'chat_id': self.tg_chat_id,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': False
            }
            
            async with self.session.post(url, json=data) as response:
                if response.status != 200:
                    response_text = await response.text()
                    logger.error(f"Failed to send Telegram notification, status: {response.status}, response: {response_text}")
        except Exception as e:
            logger.error(f"Error sending Telegram notification: {str(e)}")

    def format_url_for_display(self, url: str) -> str:
        """æ ¼å¼åŒ–URLæ˜¾ç¤º"""
        if url.startswith(self.base_url):
            display_url = url[len(self.base_url):]
            match = re.search(r'/archives/(\d+)(?:/([^/]+))?', display_url)
            if match:
                article_id = match.group(1)
                article_slug = match.group(2) or ""
                return f"/archives/{article_id} {article_slug}"
        return url

    def generate_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š"""
        total_success = sum(stats['success'] for stats in self.visit_stats.values())
        total_failed = sum(stats['failed'] for stats in self.visit_stats.values())
        total_attempts = total_success + total_failed
        success_rate = (total_success / total_attempts * 100) if total_attempts > 0 else 0
        
        # æ¯ç¯‡æ–‡ç« çš„è®¿é—®è¯¦æƒ…
        article_details = []
        for url in sorted(self.article_urls, key=lambda x: self.visit_stats[x]['success'], reverse=True):
            stats = self.visit_stats[url]
            article_details.append(
                f"{self.format_url_for_display(url)} - "
                f"âœ…{stats['success']} âŒ{stats['failed']} "
                f"({stats['success']/(stats['success']+stats['failed'])*100:.1f}%)"
            )
        
        # æ„å»ºæŠ¥å‘Šå†…å®¹
        report_lines = [
            "ğŸ“Š <b>åšå®¢è®¿é—®æ¨¡æ‹Ÿè¯¦ç»†æŠ¥å‘Š</b>",
            f"ğŸ  <b>åšå®¢åœ°å€:</b> {self.base_url}",
            f"ğŸ•’ <b>æ‰§è¡Œæ—¶é—´:</b> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}",
            "",
            f"ğŸ” <b>å‘ç°æ–‡ç« æ•°:</b> {len(self.article_urls)}",
            f"ğŸ”„ <b>æ€»è®¿é—®æ¬¡æ•°:</b> {total_attempts}",
            f"âœ… <b>æˆåŠŸè®¿é—®:</b> {total_success} æ¬¡",
            f"âŒ <b>å¤±è´¥è®¿é—®:</b> {total_failed} æ¬¡",
            f"ğŸ“ˆ <b>æ€»æˆåŠŸç‡:</b> {success_rate:.1f}%",
            "",
            "ğŸ“ <b>æ–‡ç« è®¿é—®è¯¦æƒ…:</b>",
            *article_details[:20],  # æœ€å¤šæ˜¾ç¤º20ç¯‡æ–‡ç« è¯¦æƒ…
            "",
            f"âš¡ <b>æ‰§è¡Œç»“æœ:</b> {'âœ… æˆåŠŸ' if success_rate > 90 else 'âš ï¸ ä¸€èˆ¬' if success_rate > 70 else 'âŒ ä¸ç†æƒ³'}",
            "",
            "ğŸ’¡ <b>å»ºè®®:</b> " + (
                "è®¿é—®æƒ…å†µéå¸¸ç†æƒ³" if success_rate > 90 else
                "è®¿é—®æƒ…å†µè‰¯å¥½ï¼Œå¯é€‚å½“å¢åŠ è®¿é—®é‡" if success_rate > 80 else
                "è®¿é—®æˆåŠŸç‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€" if success_rate > 60 else
                "è®¿é—®æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´è®¿é—®é¢‘ç‡æˆ–æ£€æŸ¥ç½‘ç»œè®¾ç½®"
            )
        ]
        
        if len(article_details) > 20:
            report_lines.insert(-4, f"...ï¼ˆå…± {len(article_details)} ç¯‡æ–‡ç« çš„è¯¦ç»†æ•°æ®ï¼‰")
        
        return "\n".join(report_lines)

async def main():
    parser = argparse.ArgumentParser(description='Typechoåšå®¢ç²¾ç¡®è®¿é—®æ¨¡æ‹Ÿå·¥å…·')
    parser.add_argument('--base-url', type=str, default='https://www.207725.xyz', help='åšå®¢åŸºç¡€URL')
    parser.add_argument('--visits', type=int, default=100, help='æ€»è®¿é—®æ¬¡æ•°')
    parser.add_argument('--max-pages', type=int, default=10, help='æœ€å¤§æŠ“å–é¡µé¢æ•°')
    parser.add_argument('--tg-bot-token', type=str, help='Telegram Bot Token')
    parser.add_argument('--tg-chat-id', type=str, help='Telegram Chat ID')
    parser.add_argument('--max-concurrent', type=int, default=5, help='æœ€å¤§å¹¶å‘è¯·æ±‚æ•°')
    args = parser.parse_args()

    crawler = TypechoCrawler(
        base_url=args.base_url,
        tg_bot_token=args.tg_bot_token,
        tg_chat_id=args.tg_chat_id
    )
    
    try:
        await crawler.init_session()
        
        # è·å–æ–‡ç« URL
        logger.info("å¼€å§‹æŠ“å–æ–‡ç« URL...")
        start_time = time.time()
        article_urls = await crawler.get_article_urls(max_pages=args.max_pages)
        elapsed = time.time() - start_time
        logger.info(f"å…±æ‰¾åˆ° {len(article_urls)} ç¯‡æ–‡ç« ï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        
        if not article_urls:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # æ¨¡æ‹Ÿè®¿é—®
        logger.info(f"å¼€å§‹æ¨¡æ‹Ÿ {args.visits} æ¬¡è®¿é—®...")
        start_time = time.time()
        await crawler.simulate_visits(total_visits=args.visits, max_concurrent=args.max_concurrent)
        elapsed = time.time() - start_time
        logger.info(f"æ¨¡æ‹Ÿè®¿é—®å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        
        # å‘é€æŠ¥å‘Š
        report = crawler.generate_report()
        logger.info("\n" + report.replace('<b>', '').replace('</b>', ''))
        
        if crawler.tg_bot_token and crawler.tg_chat_id:
            await crawler.send_telegram_notification(report)
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {str(e)}")
        if crawler.tg_bot_token and crawler.tg_chat_id:
            await crawler.send_telegram_notification(f"âš ï¸ åšå®¢è®¿é—®è„šæœ¬å‡ºé”™:\n{str(e)}")
    finally:
        await crawler.close_session()

if __name__ == '__main__':
    asyncio.run(main())
