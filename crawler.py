#!/usr/bin/env python3
import asyncio
import aiohttp
import random
import time
import sys
from bs4 import BeautifulSoup
import argparse
import logging
from typing import List, Dict, Tuple
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class TypechoCrawler:
    def __init__(self, base_url: str, tg_bot_token: str = None, tg_chat_id: str = None):
        self.base_url = base_url.rstrip('/')
        self.session = None
        self.tg_bot_token = tg_bot_token
        self.tg_chat_id = tg_chat_id
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': self.base_url,
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.visited_urls = set()
        self.success_count = 0
        self.failure_count = 0
        self.article_urls = []

    async def init_session(self):
        connector = aiohttp.TCPConnector(limit=10, force_close=True)
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=60)
        )

    async def close_session(self):
        if self.session:
            await self.session.close()

    async def fetch_url_with_retry(self, url: str, max_retries: int = 3) -> Tuple[bool, str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„URLè®¿é—®"""
        last_error = ""
        for attempt in range(max_retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return True, ""
                    last_error = f"HTTP status {response.status}"
                    if response.status == 429:  # Too Many Requests
                        wait_time = (attempt + 1) * 5
                        logger.warning(f"Rate limited, waiting {wait_time} seconds before retry...")
                        await asyncio.sleep(wait_time)
            except Exception as e:
                last_error = str(e)
            
            if attempt < max_retries - 1:
                wait_time = random.uniform(1.0, 3.0) * (attempt + 1)
                logger.warning(f"Attempt {attempt + 1} failed for {url}, retrying in {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)
        
        return False, last_error

    async def get_article_urls(self, max_pages: int = 10) -> List[str]:
        """æ”¹è¿›çš„æ–‡ç« URLæŠ“å–é€»è¾‘"""
        urls = []
        seen_urls = set()
        
        try:
            for page in range(1, max_pages + 1):
                list_url = f"{self.base_url}/index.php/archives/page/{page}" if page > 1 else self.base_url
                
                logger.info(f"Fetching articles from page {page}: {list_url}")
                success, error = await self.fetch_url_with_retry(list_url)
                
                if not success:
                    logger.warning(f"Failed to fetch page {page}: {error}")
                    continue
                
                async with self.session.get(list_url) as response:
                    html = await response.text()
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # æ”¹è¿›çš„é€‰æ‹©å™¨ï¼Œé€‚åº”Typechoçš„ä¸åŒä¸»é¢˜
                link_selectors = [
                    '.post-title a',  # å¸¸è§çš„é€‰æ‹©å™¨
                    'h2.title a',     # å¦ä¸€ç§å¸¸è§é€‰æ‹©å™¨
                    'article header h2 a',  # æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                    'a[href*="/archives/"]'  # é€šç”¨åŒ¹é…
                ]
                
                found_links = False
                for selector in link_selectors:
                    links = soup.select(selector)
                    if links:
                        found_links = True
                        for link in links:
                            href = link.get('href', '').strip()
                            if not href:
                                continue
                                
                            # è§„èŒƒåŒ–URL
                            if href.startswith('/'):
                                href = f"{self.base_url}{href}"
                            elif not href.startswith(('http://', 'https://')):
                                href = f"{self.base_url}/{href}"
                            
                            # ç¡®ä¿æ˜¯æ–‡ç« URLä¸”æœªé‡å¤
                            if (href.startswith(f"{self.base_url}/index.php/archives/") and 
                               href not in seen_urls and
                               re.match(r'.*/archives/\d+/.*', href):
                                seen_urls.add(href)
                                urls.append(href)
                                logger.debug(f"Found article: {href}")
                        
                        if found_links:
                            break
                
                if not found_links:
                    logger.warning(f"No article links found on page {page} with selectors")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                next_page = soup.select_one('.next, .page-navigator a:contains("ä¸‹ä¸€é¡µ"), a[rel="next"]')
                if not next_page:
                    logger.info(f"No more pages found after page {page}")
                    break
                
                # éšæœºå»¶è¿Ÿé˜²æ­¢è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(random.uniform(1.0, 3.0))
        
        except Exception as e:
            logger.error(f"Error fetching article URLs: {str(e)}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œä½¿ç”¨é»˜è®¤çš„å‡ ç¯‡æ–‡ç« 
        if not urls:
            logger.warning("No articles found, using default URLs")
            urls = [
                f"{self.base_url}/index.php/archives/13/",
                f"{self.base_url}/index.php/archives/5/",
                f"{self.base_url}/index.php/archives/1/"
            ]
        
        # éšæœºæ‰“ä¹±URLé¡ºåº
        random.shuffle(urls)
        self.article_urls = urls
        return urls

    async def simulate_visits(self, times: int = 10, max_concurrent: int = 5):
        """æ”¹è¿›çš„æ¨¡æ‹Ÿè®¿é—®æ–¹æ³•"""
        self.success_count = 0
        self.failure_count = 0
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def visit_with_semaphore(url):
            async with semaphore:
                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(1.0, 5.0)
                await asyncio.sleep(delay)
                
                success, error = await self.fetch_url_with_retry(url)
                if success:
                    self.success_count += 1
                else:
                    self.failure_count += 1
                    logger.warning(f"Failed to visit {url}: {error}")
        
        # åˆ›å»ºè®¿é—®ä»»åŠ¡
        tasks = []
        for _ in range(times):
            url = random.choice(self.article_urls)
            tasks.append(visit_with_semaphore(url))
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*tasks)

    async def send_telegram_notification(self, message: str):
        """å‘é€Telegramé€šçŸ¥"""
        if not self.tg_bot_token or not self.tg_chat_id:
            logger.warning("Telegram bot token or chat ID not provided, skipping notification")
            return
        
        try:
            url = f"https://api.telegram.org/bot{self.tg_bot_token}/sendMessage"
            data = {
                'chat_id': self.tg_chat_id,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': False
            }
            
            async with self.session.post(url, json=data) as response:
                if response.status != 200:
                    response_text = await response.text()
                    logger.error(f"Failed to send Telegram notification, status: {response.status}, response: {response_text}")
        except Exception as e:
            logger.error(f"Error sending Telegram notification: {str(e)}")

    def format_url_for_display(self, url: str) -> str:
        """æ ¼å¼åŒ–URLæ˜¾ç¤º"""
        if url.startswith(self.base_url):
            display_url = url[len(self.base_url):]
            # æå–æ–‡ç« æ ‡é¢˜æˆ–ID
            match = re.search(r'/archives/(\d+)(?:/([^/]+))?', display_url)
            if match:
                article_id = match.group(1)
                article_slug = match.group(2) or ""
                return f"æ–‡ç«  {article_id} {article_slug}"
        return url

    def generate_report(self) -> str:
        """ç”Ÿæˆæ›´è¯¦ç»†çš„æŠ¥å‘Š"""
        total_attempts = self.success_count + self.failure_count
        success_rate = (self.success_count / total_attempts * 100) if total_attempts > 0 else 0
        
        # ç»Ÿè®¡å„æ–‡ç« è¢«è®¿é—®æ¬¡æ•°
        article_stats = "\n".join(
            f"{i+1}. {self.format_url_for_display(url)}"
            for i, url in enumerate(self.article_urls[:10])  # åªæ˜¾ç¤ºå‰10ç¯‡æ–‡ç« 
        )
        
        if len(self.article_urls) > 10:
            article_stats += f"\n...ï¼ˆå…± {len(self.article_urls)} ç¯‡æ–‡ç« ï¼‰"
        
        report_lines = [
            "ğŸ“Š <b>åšå®¢è®¿é—®æ¨¡æ‹ŸæŠ¥å‘Š - ä¼˜åŒ–ç‰ˆ</b>",
            f"ğŸ  <b>åšå®¢åœ°å€:</b> {self.base_url}",
            f"ğŸ•’ <b>æ‰§è¡Œæ—¶é—´:</b> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}",
            "",
            f"ğŸ” <b>å‘ç°æ–‡ç« æ•°:</b> {len(self.article_urls)}",
            f"ğŸ”„ <b>æ€»è®¿é—®æ¬¡æ•°:</b> {total_attempts}",
            f"âœ… <b>æˆåŠŸè®¿é—®:</b> {self.success_count} æ¬¡",
            f"âŒ <b>å¤±è´¥è®¿é—®:</b> {self.failure_count} æ¬¡",
            f"ğŸ“ˆ <b>æˆåŠŸç‡:</b> {success_rate:.1f}%",
            "",
            "ğŸ“ <b>éƒ¨åˆ†æ–‡ç« åˆ—è¡¨:</b>",
            article_stats,
            "",
            "âš¡ <b>æ‰§è¡Œç»“æœ:</b> " + ("âœ… æˆåŠŸ" if success_rate > 70 else "âš ï¸ ä¸€èˆ¬" if success_rate > 50 else "âŒ ä¸ç†æƒ³"),
            "",
            "ğŸ’¡ <b>å»ºè®®:</b> " + (
                "è®¿é—®æƒ…å†µè‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼" if success_rate > 80 else
                "è®¿é—®æˆåŠŸç‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€" if success_rate > 60 else
                "è®¿é—®æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´è®¿é—®é¢‘ç‡æˆ–æ£€æŸ¥ç½‘ç»œè®¾ç½®"
            )
        ]
        
        return "\n".join(report_lines)

async def main():
    parser = argparse.ArgumentParser(description='Typechoåšå®¢æ¨¡æ‹Ÿè®¿é—®å·¥å…· - ä¼˜åŒ–ç‰ˆ')
    parser.add_argument('--base-url', type=str, default='https://www.207725.xyz', help='åšå®¢åŸºç¡€URL')
    parser.add_argument('--times', type=int, default=30, help='æ¨¡æ‹Ÿè®¿é—®æ¬¡æ•°')
    parser.add_argument('--max-pages', type=int, default=5, help='æœ€å¤§æŠ“å–é¡µé¢æ•°')
    parser.add_argument('--tg-bot-token', type=str, help='Telegram Bot Token')
    parser.add_argument('--tg-chat-id', type=str, help='Telegram Chat ID')
    parser.add_argument('--max-concurrent', type=int, default=3, help='æœ€å¤§å¹¶å‘è¯·æ±‚æ•°')
    args = parser.parse_args()

    crawler = TypechoCrawler(
        base_url=args.base_url,
        tg_bot_token=args.tg_bot_token,
        tg_chat_id=args.tg_chat_id
    )
    
    try:
        await crawler.init_session()
        
        # è·å–æ–‡ç« URL
        logger.info("å¼€å§‹æŠ“å–æ–‡ç« URL...")
        article_urls = await crawler.get_article_urls(max_pages=args.max_pages)
        logger.info(f"å…±æ‰¾åˆ° {len(article_urls)} ç¯‡æ–‡ç« ")
        
        if not article_urls:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # æ¨¡æ‹Ÿè®¿é—®
        logger.info(f"å¼€å§‹æ¨¡æ‹Ÿ {args.times} æ¬¡è®¿é—®...")
        start_time = time.time()
        await crawler.simulate_visits(times=args.times, max_concurrent=args.max_concurrent)
        elapsed = time.time() - start_time
        logger.info(f"æ¨¡æ‹Ÿè®¿é—®å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        
        # å‘é€æŠ¥å‘Š
        report = crawler.generate_report()
        logger.info("\n" + report.replace('<b>', '').replace('</b>', ''))
        
        if crawler.tg_bot_token and crawler.tg_chat_id:
            await crawler.send_telegram_notification(report)
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {str(e)}")
        if crawler.tg_bot_token and crawler.tg_chat_id:
            await crawler.send_telegram_notification(f"âš ï¸ åšå®¢è®¿é—®è„šæœ¬å‡ºé”™:\n{str(e)}")
    finally:
        await crawler.close_session()

if __name__ == '__main__':
    asyncio.run(main())
