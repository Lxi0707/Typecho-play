import os
import re
import random
import asyncio
import aiohttp
from datetime import datetime
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from urllib.parse import urljoin
from telegram import Bot
from telegram.error import TelegramError
from tqdm import tqdm

class EnhancedTypechoCrawler:
    def __init__(self):
        self.ua = UserAgent()
        self.log_file = 'crawl_log.txt'
        self.output_dir = 'typecho_output'
        self.blog_url = os.getenv('BLOG_URL', 'https://www.207725.xyz')
        self.start_page = int(os.getenv('START_PAGE', 1))
        self.max_pages = int(os.getenv('MAX_PAGES', 1))
        self.session = None
        self._init_dirs()
        
    def _init_dirs(self):
        """åˆå§‹åŒ–ç›®å½•"""
        os.makedirs(self.output_dir, exist_ok=True)
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(f"Typechoåšå®¢çˆ¬è™«æ—¥å¿— - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åšå®¢URL: {self.blog_url}\n")
            f.write(f"å¼€å§‹é¡µç : {self.start_page}, æœ€å¤§é¡µæ•°: {self.max_pages}\n\n")
    
    async def _log(self, message):
        """å¼‚æ­¥è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] {message}\n"
        print(log_entry.strip())
        async with asyncio.Lock():
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
    
    async def _get_telegram_credentials(self):
        """è·å–Telegramå‡­è¯"""
        bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        chat_id = os.getenv('TELEGRAM_CHAT_ID')
        if not bot_token or not chat_id:
            raise ValueError("Telegramå‡­è¯æœªè®¾ç½®")
        return bot_token, chat_id
    
    async def _create_session(self):
        """åˆ›å»ºaiohttpä¼šè¯"""
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=5, force_close=True),
            headers={'Accept-Language': 'zh-CN,zh;q=0.9'},
            timeout=aiohttp.ClientTimeout(total=30)
        )
    
    async def _close_session(self):
        """å…³é—­aiohttpä¼šè¯"""
        if self.session:
            await self.session.close()
    
    async def _random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(1, 3)
        await asyncio.sleep(delay)
    
    async def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            headers = {'User-Agent': self.ua.random}
            async with self.session.get(url, headers=headers) as response:
                response.raise_for_status()
                return await response.text()
        except Exception as e:
            await self._log(f"è·å–é¡µé¢å¤±è´¥: {url} - {str(e)}")
            return None
    
    def parse_post_links(self, html):
        """è§£ææ–‡ç« é“¾æ¥"""
        soup = BeautifulSoup(html, 'lxml')
        links = []
        for article in soup.select('article.post, div.post'):
            link = article.select_one('h2 a, h1 a')
            if link and 'href' in link.attrs:
                full_url = urljoin(self.blog_url, link['href'])
                links.append(full_url)
        return links
    
    async def get_all_post_links(self):
        """è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥"""
        await self._log("å¼€å§‹æ”¶é›†æ–‡ç« é“¾æ¥...")
        post_links = []
        
        async with tqdm(total=self.max_pages, desc="æ”¶é›†æ–‡ç« é“¾æ¥") as pbar:
            for page in range(self.start_page, self.start_page + self.max_pages):
                page_url = f"{self.blog_url}/page/{page}/" if page > 1 else self.blog_url
                html = await self.fetch_page(page_url)
                if html:
                    links = self.parse_post_links(html)
                    post_links.extend(links)
                    await self._log(f"ç¬¬{page}é¡µæ‰¾åˆ°{len(links)}ç¯‡æ–‡ç« ")
                await self._random_delay()
                pbar.update(1)
        
        await self._log(f"å…±æ”¶é›†åˆ°{len(post_links)}ç¯‡æ–‡ç« é“¾æ¥")
        return post_links
    
    async def crawl_post(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        try:
            html = await self.fetch_page(url)
            if not html:
                return None, None, None
            
            soup = BeautifulSoup(html, 'lxml')
            
            # æå–æ ‡é¢˜
            title = soup.select_one('h1.post-title, h1.entry-title, h1')
            title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
            
            # æå–å†…å®¹
            article = soup.select_one('div.post-content, article.post, div.post-body')
            if article:
                for element in article(['script', 'style', 'nav', 'footer', 'iframe', 'aside']):
                    element.decompose()
                content = article.get_text('\n', strip=True)
                content = re.sub(r'\s+', ' ', content)
            else:
                content = "æ— å†…å®¹"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date = soup.select_one('time.entry-date, time.post-date, .post-meta time')
            date_text = date['datetime'] if date and 'datetime' in date.attrs else (
                date.get_text().strip() if date else "æœªçŸ¥æ—¥æœŸ"
            )
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"{self.output_dir}/{title_text[:50]}.html".replace('/', '_')
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"<h1>{title_text}</h1>\n")
                f.write(f"<p><strong>URL:</strong> <a href='{url}'>{url}</a></p>\n")
                f.write(f"<p><strong>å‘å¸ƒæ—¥æœŸ:</strong> {date_text}</p>\n")
                if article:
                    f.write(article.prettify())
                else:
                    f.write(f"<pre>{content}</pre>")
            
            return title_text, content, url
        
        except Exception as e:
            await self._log(f"å¤„ç†æ–‡ç« å¤±è´¥: {url} - {str(e)}")
            return None, None, None
    
    async def send_telegram_notification(self, posts):
        """å‘é€Telegramé€šçŸ¥"""
        try:
            bot_token, chat_id = await self._get_telegram_credentials()
            bot = Bot(token=bot_token)
            
            message = "ğŸ“š *åšå®¢æ–‡ç« æ›´æ–°é€šçŸ¥*\n\n"
            message += f"*åšå®¢:* [{self.blog_url}]({self.blog_url})\n"
            message += f"*æŠ“å–æ—¶é—´:* {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
            message += f"*å‘ç°æ–‡ç« æ•°:* {len(posts)}\n\n"
            message += "*æ–‡ç« åˆ—è¡¨:*\n"
            
            for idx, (title, _, url) in enumerate(posts, 1):
                # ä¼˜åŒ–URLæ˜¾ç¤ºï¼Œåªä¿ç•™è·¯å¾„éƒ¨åˆ†
                clean_url = url.replace(self.blog_url, '')
                message += f"{idx}. [{title}]({url}) `{clean_url}`\n"
                if idx % 5 == 0:  # æ¯5æ¡æ¶ˆæ¯åˆ†å‰²ä¸€æ¬¡
                    await bot.send_message(
                        chat_id=chat_id,
                        text=message,
                        parse_mode='Markdown',
                        disable_web_page_preview=True
                    )
                    message = ""
                    await asyncio.sleep(1)
            
            if message.strip():
                await bot.send_message(
                    chat_id=chat_id,
                    text=message,
                    parse_mode='Markdown',
                    disable_web_page_preview=True
                )
            
            await self._log(f"æˆåŠŸå‘é€{len(posts)}ç¯‡æ–‡ç« é€šçŸ¥")
            return True
        
        except Exception as e:
            await self._log(f"å‘é€é€šçŸ¥å¤±è´¥: {str(e)}")
            return False
    
    async def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        await self._create_session()
        try:
            post_links = await self.get_all_post_links()
            crawled_posts = []
            
            await self._log("å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹...")
            for url in tqdm(post_links, desc="çˆ¬å–æ–‡ç« "):
                title, content, url = await self.crawl_post(url)
                if title and content and url:
                    crawled_posts.append((title, content, url))
                await self._random_delay()
            
            if crawled_posts:
                await self.send_telegram_notification(crawled_posts)
            else:
                await self._log("æ²¡æœ‰æ‰¾åˆ°å¯çˆ¬å–çš„æ–‡ç« ")
            
        finally:
            await self._close_session()

async def main():
    crawler = EnhancedTypechoCrawler()
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
