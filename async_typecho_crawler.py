import os
import sys
import re
import random
import asyncio
import aiohttp
from datetime import datetime
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from telegram import Bot
from telegram.error import TelegramError

class AsyncTypechoCrawler:
    def __init__(self):
        self.ua = UserAgent()
        self.log_file = 'async_crawl_log.txt'
        self.connector = None
        self.session = None
        self._init_log()
        
    def _init_log(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(f"å¼‚æ­¥Typechoçˆ¬è™«æ—¥å¿— - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    async def _log(self, message):
        """å¼‚æ­¥è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] {message}\n"
        print(log_entry.strip())
        async with asyncio.Lock():
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
    
    async def _get_telegram_credentials(self):
        """è·å– Telegram å‡­è¯"""
        bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        chat_id = os.getenv('TELEGRAM_CHAT_ID')
        
        if not bot_token or not chat_id:
            raise ValueError("Telegram å‡­è¯æœªåœ¨ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ°")
        
        return bot_token, chat_id
    
    def _clean_content(self, content):
        """æ¸…ç†æ–‡ç« å†…å®¹"""
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'post-(footer|tags|copyright)', '', content)
        return content.strip()
    
    async def _random_delay(self):
        """éšæœºå»¶è¿Ÿ 1-3 ç§’"""
        delay = random.uniform(1, 3)
        await self._log(f"éšæœºå»¶è¿Ÿ {delay:.2f} ç§’...")
        await asyncio.sleep(delay)
    
    async def _create_session(self):
        """åˆ›å»ºaiohttpä¼šè¯"""
        self.connector = aiohttp.TCPConnector(
            limit=5,  # åŒæ—¶æœ€å¤§è¿æ¥æ•°
            force_close=True,
            enable_cleanup_closed=True,
            ttl_dns_cache=300
        )
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            headers={'Accept-Language': 'zh-CN,zh;q=0.9'},
            timeout=aiohttp.ClientTimeout(total=30)
        )
    
    async def _close_session(self):
        """å…³é—­aiohttpä¼šè¯"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    async def _simulate_visit(self, url, visit_num):
        """æ¨¡æ‹Ÿå•æ¬¡å¼‚æ­¥è®¿é—®"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Referer': 'https://www.207725.xyz' if visit_num == 1 else url
            }
            
            await self._log(f"æ¨¡æ‹Ÿè®¿é—® #{visit_num} - ä½¿ç”¨ UA: {headers['User-Agent']}")
            await self._random_delay()
            
            async with self.session.get(url, headers=headers) as response:
                response.raise_for_status()
                return await response.text()
        
        except Exception as e:
            await self._log(f"æ¨¡æ‹Ÿè®¿é—® #{visit_num} å¤±è´¥: {str(e)}")
            return None
    
    async def crawl_typecho_post(self, url, visit_count=1):
        """å¼‚æ­¥çˆ¬å– Typecho åšå®¢æ–‡ç« """
        try:
            visit_count = min(max(int(visit_count), 1), 5)  # é™åˆ¶1-5æ¬¡
            
            await self._log(f"å¼€å§‹å¼‚æ­¥çˆ¬å–: {url} (æ¨¡æ‹Ÿè®¿é—®æ¬¡æ•°: {visit_count})")
            await self._create_session()
            
            # åˆ›å»ºæ‰€æœ‰è®¿é—®ä»»åŠ¡
            tasks = [self._simulate_visit(url, i+1) for i in range(visit_count)]
            html_contents = await asyncio.gather(*tasks)
            
            # è¿‡æ»¤æ‰å¤±è´¥çš„è®¿é—®
            html_contents = [html for html in html_contents if html is not None]
            
            if not html_contents:
                return "è®¿é—®å¤±è´¥", ""
            
            # ä½¿ç”¨æœ€åä¸€æ¬¡æˆåŠŸè®¿é—®çš„ç»“æœ
            soup = BeautifulSoup(html_contents[-1], 'lxml')
            
            # Typecho æ–‡ç« æ ‡é¢˜
            title = soup.select_one('h1.post-title, h1')
            title_text = title.get_text().strip() if title else "æœªæ‰¾åˆ°æ ‡é¢˜"
            
            # Typecho æ–‡ç« å†…å®¹
            article = soup.select_one('div.post-content, article.post, div.post-body')
            
            if article:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for element in article(['script', 'style', 'nav', 'footer', 'iframe', 'aside', 'div.post-meta', 'div.post-footer']):
                    element.decompose()
                
                content = self._clean_content(article.get_text('\n', strip=True))
            else:
                content = "æœªæ‰¾åˆ°æ–‡ç« å†…å®¹"
            
            # ä¿å­˜ç»“æœåˆ° HTML æ–‡ä»¶
            with open('typecho_output.html', 'w', encoding='utf-8') as f:
                f.write(f"<!DOCTYPE html>\n<html>\n<head>\n<meta charset='utf-8'>\n")
                f.write(f"<title>{title_text}</title>\n</head>\n<body>\n")
                f.write(f"<h1>{title_text}</h1>\n")
                f.write(f"<p><strong>URL:</strong> <a href='{url}'>{url}</a></p>\n")
                f.write(f"<p><strong>æŠ“å–æ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n")
                f.write(f"<p><strong>æ¨¡æ‹Ÿè®¿é—®æ¬¡æ•°:</strong> {visit_count}</p>\n")
                if article:
                    f.write(article.prettify())
                else:
                    f.write(f"<pre>{content}</pre>")
                f.write("\n</body>\n</html>")
            
            return title_text, content
        
        except Exception as e:
            await self._log(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return f"å¤„ç†é”™è¯¯: {str(e)}", ""
        finally:
            await self._close_session()
    
    async def send_telegram_notification(self, title, content, url, visit_count):
        """å¼‚æ­¥å‘é€ Telegram é€šçŸ¥"""
        try:
            bot_token, chat_id = await self._get_telegram_credentials()
            bot = Bot(token=bot_token)
            
            message = f"ğŸ“ *å¼‚æ­¥åšå®¢æ–‡ç« æŠ“å–å®Œæˆ*\n\n"
            message += f"*æ ‡é¢˜:* {title}\n"
            message += f"*é“¾æ¥:* [ç‚¹å‡»æŸ¥çœ‹]({url})\n"
            message += f"*æ¨¡æ‹Ÿè®¿é—®æ¬¡æ•°:* {visit_count}\n\n"
            
            # å‘é€æ ‡é¢˜æ¶ˆæ¯
            await bot.send_message(
                chat_id=chat_id,
                text=message,
                parse_mode='Markdown',
                disable_web_page_preview=False
            )
            
            # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œåˆ†å¼€å‘é€
            chunk_size = 3000
            content_chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
            
            # å‘é€å†…å®¹åˆ†å—
            for i, chunk in enumerate(content_chunks, 1):
                chunk_msg = f"*å†…å®¹ (éƒ¨åˆ† {i}/{len(content_chunks)}):*\n{chunk}"
                await bot.send_message(
                    chat_id=chat_id,
                    text=chunk_msg,
                    parse_mode='Markdown'
                )
                await asyncio.sleep(1)  # é¿å…å‘é€è¿‡å¿«
            
            await self._log("Telegram é€šçŸ¥å‘é€æˆåŠŸ")
            return True
        
        except TelegramError as e:
            await self._log(f"å‘é€ Telegram é€šçŸ¥å¤±è´¥: {str(e)}")
            return False
        except Exception as e:
            await self._log(f"å‘é€é€šçŸ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

async def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python async_typecho_crawler.py <url> [visit_count]")
        return
    
    target_url = sys.argv[1]
    visit_count = sys.argv[2] if len(sys.argv) > 2 else 1
    
    crawler = AsyncTypechoCrawler()
    
    title, content = await crawler.crawl_typecho_post(target_url, visit_count)
    
    if title and content:
        success = await crawler.send_telegram_notification(title, content, target_url, visit_count)
        await crawler._log(f"çˆ¬å–{'å¹¶é€šçŸ¥' if success else 'ä½†é€šçŸ¥å¤±è´¥'}")
    else:
        await crawler._log("çˆ¬å–å¤±è´¥ï¼Œæœªå‘é€é€šçŸ¥")

if __name__ == "__main__":
    asyncio.run(main())
