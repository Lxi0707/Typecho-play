#!/usr/bin/env python3
"""
Typechoåšå®¢ä¸“ä¸šè®¿é—®è„šæœ¬V3.2
æ›´æ–°å†…å®¹ï¼š
1. ä¿®å¤GitHub Actionså‚æ•°ä¼ é€’é—®é¢˜
2. ä¿æŒæ‰€æœ‰åŸæœ‰åŠŸèƒ½ä¸å˜
3. å¢å¼ºé”™è¯¯å¤„ç†
"""

import asyncio
import aiohttp
import random
import argparse
from datetime import datetime
import logging
import os
import sys
from typing import Dict, List, Tuple, Optional

# å…¨å±€é…ç½®
CONFIG = {
    'blog_url': 'https://www.207725.xyz',
    'posts_file': 'posts.txt',
    'telegram_timeout': 15,
    'request_timeout': 20,
    'min_delay': 1.0,
    'max_delay': 3.0,
    'max_retries': 2,
    'conn_limit': 10,
    'req_per_url': 3,  # é»˜è®¤æ¯ä¸ªå¿…åˆ·URLè®¿é—®æ¬¡æ•°
    'default_urls': [
        '/index.php/archives/13/',
        '/index.php/archives/5/'
    ]
}

# ç”¨æˆ·ä»£ç†æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
]

class TypechoVisitor:
    def __init__(self, normal_visits: int, required_visits: Optional[int] = None):
        self.normal_visits = normal_visits
        self.required_visits = required_visits if required_visits is not None else CONFIG['req_per_url']
        self.stats = {
            'required': {'success': 0, 'failure': 0, 'urls': {}},
            'normal': {'success': 0, 'failure': 0, 'urls': {}}
        }
        self.start_time = datetime.now()
        self._setup_logging()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ | æ™®é€šè®¿é—®: {normal_visits}æ¬¡ | å¿…åˆ·è®¿é—®: {self.required_visits}æ¬¡/URL")

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        global logger
        logger = logging.getLogger('typecho_visitor')
        logger.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        file_handler = logging.FileHandler('visit.log', encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    async def _get_connector(self):
        """åˆ›å»ºä¼˜åŒ–çš„TCPè¿æ¥å™¨"""
        return aiohttp.TCPConnector(
            limit=CONFIG['conn_limit'],
            force_close=False,
            enable_cleanup_closed=True,
            ssl=False
        )

    async def _get_urls(self) -> Tuple[List[str], List[str]]:
        """è·å–URLåˆ—è¡¨ï¼š(å¿…åˆ·URLs, æ™®é€šURLs)"""
        required = await self._load_required_urls()
        normal = await self._discover_normal_urls()
        return required, normal or self._get_fallback_urls()

    async def _load_required_urls(self) -> List[str]:
        """åŠ è½½å¿…åˆ·URLåˆ—è¡¨"""
        try:
            if not os.path.exists(CONFIG['posts_file']):
                with open(CONFIG['posts_file'], 'w', encoding='utf-8') as f:
                    f.write('\n'.join(CONFIG['default_urls']))
                logger.info(f"å·²åˆ›å»ºé»˜è®¤ {CONFIG['posts_file']}")
            
            with open(CONFIG['posts_file'], 'r', encoding='utf-8') as f:
                return [self._normalize_url(line.strip()) for line in f if line.strip()]
        except Exception as e:
            logger.error(f"åŠ è½½å¿…åˆ·URLå¤±è´¥: {str(e)}")
            return []

    async def _discover_normal_urls(self) -> List[str]:
        """å‘ç°æ™®é€šæ–‡ç« URL"""
        try:
            async with aiohttp.ClientSession(
                connector=await self._get_connector(),
                timeout=aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
            ) as session:
                async with session.get(
                    CONFIG['blog_url'],
                    headers={'User-Agent': random.choice(USER_AGENTS)}
                ) as resp:
                    if resp.status == 200:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(await resp.text(), 'html.parser')
                        return list({
                            self._normalize_url(a['href'])
                            for a in soup.find_all('a', href=True)
                            if '/archives/' in a['href'] and '#' not in a['href']
                        })
        except Exception as e:
            logger.error(f"å‘ç°æ–‡ç« å¼‚å¸¸: {str(e)}")
        return []

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if url.startswith('http'):
            return url.split('#')[0]
        return f"{CONFIG['blog_url']}{url if url.startswith('/') else '/' + url}".split('#')[0]

    def _get_fallback_urls(self) -> List[str]:
        """è·å–å¤‡ç”¨URLåˆ—è¡¨"""
        return [self._normalize_url(url) for url in CONFIG['default_urls']]

    async def _visit(self, session: aiohttp.ClientSession, url: str, is_required: bool):
        """æ‰§è¡Œå•æ¬¡è®¿é—®ï¼ˆå«é‡è¯•æœºåˆ¶ï¼‰"""
        key = 'required' if is_required else 'normal'
        
        for attempt in range(CONFIG['max_retries'] + 1):
            try:
                await asyncio.sleep(random.uniform(CONFIG['min_delay'], CONFIG['max_delay']))
                
                async with session.get(
                    url,
                    headers={
                        'User-Agent': random.choice(USER_AGENTS),
                        'Accept': 'text/html,application/xhtml+xml',
                        'Referer': CONFIG['blog_url']
                    },
                    timeout=aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
                ) as resp:
                    if resp.status == 200:
                        self.stats[key]['success'] += 1
                        self.stats[key]['urls'][url] = self.stats[key]['urls'].get(url, 0) + 1
                        return True
                    logger.warning(f"è®¿é—®å¤±è´¥[{attempt+1}]: {url} (HTTP {resp.status})")
            except Exception as e:
                logger.warning(f"è®¿é—®å¼‚å¸¸[{attempt+1}]: {url} - {type(e).__name__}")
            
            if attempt < CONFIG['max_retries']:
                await asyncio.sleep(1)
        
        self.stats[key]['failure'] += 1
        return False

    async def _run_required_visits(self, urls: List[str]):
        """æ‰§è¡Œå¿…åˆ·URLè®¿é—®"""
        if not urls:
            return logger.warning("æ²¡æœ‰å¿…åˆ·URLï¼Œè·³è¿‡è¯¥é˜¶æ®µ")
            
        logger.info(f"å¼€å§‹å¿…åˆ·è®¿é—® | URLæ•°: {len(urls)} | æ¯URLæ¬¡æ•°: {self.required_visits}")
        
        async with aiohttp.ClientSession(
            connector=await self._get_connector(),
            timeout=aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
        ) as session:
            tasks = []
            for url in urls:
                for _ in range(self.required_visits):
                    tasks.append(self._visit(session, url, True))
            
            await asyncio.gather(*tasks)

    async def _run_normal_visits(self, urls: List[str]):
        """æ‰§è¡Œæ™®é€šURLè®¿é—®"""
        if self.normal_visits <= 0:
            return logger.info("æ™®é€šè®¿é—®æ¬¡æ•°ä¸º0ï¼Œè·³è¿‡è¯¥é˜¶æ®µ")
            
        if not urls:
            return logger.error("æ²¡æœ‰å¯ç”¨çš„æ™®é€šURL")
            
        base_visits = self.normal_visits // len(urls)
        extra_visits = self.normal_visits % len(urls)
        
        logger.info(f"å¼€å§‹æ™®é€šè®¿é—® | URLæ•°: {len(urls)} | æ€»æ¬¡æ•°: {self.normal_visits}")
        
        async with aiohttp.ClientSession(
            connector=await self._get_connector(),
            timeout=aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
        ) as session:
            tasks = []
            for i, url in enumerate(urls):
                visits = base_visits + (1 if i < extra_visits else 0)
                for _ in range(visits):
                    tasks.append(self._visit(session, url, False))
            
            await asyncio.gather(*tasks)

    def _generate_report(self) -> str:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        duration = (datetime.now() - self.start_time).total_seconds()
        req = self.stats['required']
        norm = self.stats['normal']
        
        def format_urls(urls: Dict[str, int], limit: int = 15) -> str:
            sorted_items = sorted(urls.items(), key=lambda x: -x[1])[:limit]
            return '\n'.join(
                f"  - {url.replace(CONFIG['blog_url'], ''):<35}: {count}æ¬¡"
                for url, count in sorted_items
            )
        
        return (
            "ğŸ“Š Typechoè®¿é—®æŠ¥å‘Š\n\n"
            f"â±ï¸ æ€»è€—æ—¶: {duration:.1f}ç§’\n"
            f"ğŸŒ åšå®¢åœ°å€: {CONFIG['blog_url']}\n\n"
            "ğŸ”´ å¿…åˆ·è®¿é—®ç»Ÿè®¡:\n"
            f"  â€¢ æˆåŠŸ: {req['success']}æ¬¡\n"
            f"  â€¢ å¤±è´¥: {req['failure']}æ¬¡\n"
            f"  â€¢ æˆåŠŸç‡: {req['success']/(req['success']+req['failure'])*100:.1f}%\n\n"
            "ğŸŸ¢ æ™®é€šè®¿é—®ç»Ÿè®¡:\n"
            f"  â€¢ æˆåŠŸ: {norm['success']}æ¬¡\n"
            f"  â€¢ å¤±è´¥: {norm['failure']}æ¬¡\n"
            f"  â€¢ æˆåŠŸç‡: {norm['success']/self.normal_visits*100:.1f}%\n\n"
            "ğŸ“Œ å¿…åˆ·URLè®¿é—®è¯¦æƒ…:\n"
            f"{format_urls(req['urls'])}\n\n"
            "ğŸ“ æ™®é€šURLè®¿é—®TOP15:\n"
            f"{format_urls(norm['urls'])}"
        )

    async def _send_report(self):
        """å‘é€TelegramæŠ¥å‘Š"""
        report = self._generate_report()
        logger.info("\n" + report)
        
        if not (token := os.getenv('TELEGRAM_BOT_TOKEN')) or not (chat_id := os.getenv('TELEGRAM_CHAT_ID')):
            return logger.warning("æœªé…ç½®Telegramé€šçŸ¥")
            
        try:
            async with aiohttp.ClientSession(
                connector=await self._get_connector(),
                timeout=aiohttp.ClientTimeout(total=CONFIG['telegram_timeout'])
            ) as session:
                await session.post(
                    f"https://api.telegram.org/bot{token}/sendMessage",
                    json={
                        'chat_id': chat_id,
                        'text': report,
                        'parse_mode': 'Markdown',
                        'disable_web_page_preview': True
                    }
                )
        except Exception as e:
            logger.error(f"å‘é€æŠ¥å‘Šå¤±è´¥: {str(e)}")

    async def execute(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        required_urls, normal_urls = await self._get_urls()
        
        await self._run_required_visits(required_urls)
        await self._run_normal_visits(normal_urls)
        
        await self._send_report()
        logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

def main():
    parser = argparse.ArgumentParser(
        description="Typechoåšå®¢ä¸“ä¸šè®¿é—®è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '-n', '--normal-visits',
        type=int,
        default=500,
        help='æ™®é€šè®¿é—®ç›®æ ‡æ¬¡æ•°ï¼ˆé»˜è®¤500ï¼‰'
    )
    parser.add_argument(
        '-r', '--required-visits',
        type=int,
        help='æ¯ä¸ªå¿…åˆ·URLçš„è®¿é—®æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰'
    )
    args = parser.parse_args()
    
    visitor = TypechoVisitor(
        normal_visits=args.normal_visits,
        required_visits=args.required_visits
    )
    asyncio.run(visitor.execute())

if __name__ == '__main__':
    main()
