name: Auto Crawl Blog - Optimized

on:
  schedule:
    # 每天UTC时间3点运行 (北京时间11点)
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      times:
        description: 'Number of visits to simulate'
        required: false
        default: '30'
        type: string
      max_pages:
        description: 'Max pages to crawl for articles'
        required: false
        default: '5'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiohttp beautifulsoup4
    
    - name: Run optimized crawler
      env:
        TG_BOT_TOKEN: ${{ secrets.TG_BOT_TOKEN }}
        TG_CHAT_ID: ${{ secrets.TG_CHAT_ID }}
      run: |
        python optimized_crawler.py \
          --base-url "https://www.207725.xyz" \
          --times ${{ github.event.inputs.times || 30 }} \
          --max-pages ${{ github.event.inputs.max_pages || 5 }} \
          --tg-bot-token "$TG_BOT_TOKEN" \
          --tg-chat-id "$TG_CHAT_ID" \
          --max-concurrent 3
