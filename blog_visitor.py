#!/usr/bin/env python3
import asyncio
import aiohttp
import random
import argparse
from datetime import datetime
import logging
import os
import sys
from typing import Dict, List

# é…ç½®éƒ¨åˆ†
BLOG_URL = "https://www.207725.xyz"
POSTS_FILE = "posts.txt"  # å¿…åˆ·URLåˆ—è¡¨æ–‡ä»¶
TELEGRAM_TIMEOUT = 10  # Telegramé€šçŸ¥è¶…æ—¶(ç§’)

# ç”¨æˆ·ä»£ç†åˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
]

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('visitor.log')
    ]
)
logger = logging.getLogger(__name__)

class BlogVisitor:
    def __init__(self, total_visits: int):
        self.total_visits = total_visits
        self.success_count = 0
        self.failure_count = 0
        self.required_stats = {'success': 0, 'failure': 0}
        self.visited_urls: Dict[str, int] = {}
        self.required_urls: Dict[str, int] = {}
        self.session = None
        self.article_urls = []
        self.required_article_urls = []
        
        # ç¯å¢ƒæ£€æŸ¥
        logger.info(f"Python {sys.version.split()[0]} on {sys.platform}")
        logger.info(f"Initializing with {total_visits} normal visits")

    async def load_required_urls(self) -> bool:
        """ä»posts.txtåŠ è½½å¿…åˆ·URLåˆ—è¡¨"""
        try:
            if not os.path.exists(POSTS_FILE):
                default_urls = [
                    f"{BLOG_URL}/index.php/archives/13/",
                    f"{BLOG_URL}/index.php/archives/5/"
                ]
                with open(POSTS_FILE, 'w') as f:
                    f.write('\n'.join(default_urls))
                logger.warning(f"Created {POSTS_FILE} with default URLs")
            
            with open(POSTS_FILE, 'r') as f:
                urls = [line.strip() for line in f if line.strip()]
                self.required_article_urls = [
                    url if url.startswith('http') else f"{BLOG_URL}{url}"
                    for url in urls
                ]
            
            logger.info(f"Loaded {len(self.required_article_urls)} required URLs")
            return True
        except Exception as e:
            logger.error(f"Failed to load required URLs: {str(e)}")
            return False

    async def fetch_article_urls(self) -> List[str]:
        """ä»åšå®¢é¦–é¡µè·å–æ–‡ç« é“¾æ¥"""
        try:
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
            }
            
            timeout = aiohttp.ClientTimeout(total=15)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(BLOG_URL, headers=headers) as response:
                    if response.status == 200:
                        from bs4 import BeautifulSoup
                        text = await response.text()
                        soup = BeautifulSoup(text, 'html.parser')
                        
                        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
                        article_links = []
                        for selector in ['a[href*="/archives/"]', '.post-title a', 'article a']:
                            article_links.extend([
                                link['href'] for link in soup.select(selector) 
                                if link.has_attr('href')
                            ])
                        
                        # å»é‡å¹¶è§„èŒƒåŒ–URL
                        unique_links = []
                        seen = set()
                        for link in article_links:
                            if link.startswith('/'):
                                full_url = f"{BLOG_URL}{link}"
                            elif link.startswith(BLOG_URL):
                                full_url = link
                            else:
                                continue
                            
                            if full_url not in seen:
                                seen.add(full_url)
                                unique_links.append(full_url)
                        
                        return unique_links or self.get_fallback_urls()
                    else:
                        logger.warning(f"Fetch failed with status: {response.status}")
                        return self.get_fallback_urls()
        except Exception as e:
            logger.error(f"Error fetching articles: {str(e)}")
            return self.get_fallback_urls()

    def get_fallback_urls(self) -> List[str]:
        """è·å–å¤‡ç”¨URLåˆ—è¡¨"""
        return [
            f"{BLOG_URL}/index.php/archives/13/",
            f"{BLOG_URL}/index.php/archives/5/",
            f"{BLOG_URL}/index.php/archives/1/"
        ]

    async def visit_url(self, url: str, is_required: bool = False):
        """è®¿é—®å•ä¸ªURL"""
        try:
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Referer": BLOG_URL,
                "DNT": "1",
            }
            
            await asyncio.sleep(random.uniform(0.5, 2.5))
            
            timeout = aiohttp.ClientTimeout(total=10)
            async with self.session.get(url, headers=headers, timeout=timeout) as response:
                if response.status == 200:
                    if is_required:
                        self.required_stats['success'] += 1
                        self.required_urls[url] = self.required_urls.get(url, 0) + 1
                    else:
                        self.success_count += 1
                        self.visited_urls[url] = self.visited_urls.get(url, 0) + 1
                    logger.debug(f"Visited: {url}")
                else:
                    if is_required:
                        self.required_stats['failure'] += 1
                    else:
                        self.failure_count += 1
                    logger.warning(f"Failed {url} (HTTP {response.status})")
        except Exception as e:
            if is_required:
                self.required_stats['failure'] += 1
            else:
                self.failure_count += 1
            logger.error(f"Error visiting {url}: {str(e)}")

    async def run_required_visits(self):
        """æ‰§è¡Œå¿…åˆ·URLè®¿é—®"""
        if not self.required_article_urls:
            logger.warning("No required URLs to visit")
            return
            
        logger.info(f"Starting required visits for {len(self.required_article_urls)} URLs")
        
        tasks = []
        async with aiohttp.ClientSession() as self.session:
            for url in self.required_article_urls:
                tasks.append(self.visit_url(url, is_required=True))
            
            await asyncio.gather(*tasks)
        
        logger.info(f"Required visits completed: {self.required_stats['success']} success, {self.required_stats['failure']} failure")

    async def run_normal_visits(self):
        """æ‰§è¡Œæ™®é€šè®¿é—®"""
        if self.total_visits <= 0:
            logger.info("Skipping normal visits (count <= 0)")
            return
            
        self.article_urls = await self.fetch_article_urls()
        if not self.article_urls:
            logger.error("No articles found for normal visits")
            return
            
        logger.info(f"Starting {self.total_visits} normal visits across {len(self.article_urls)} URLs")
        
        visits_per_url = self.total_visits // len(self.article_urls)
        remainder = self.total_visits % len(self.article_urls)
        
        tasks = []
        async with aiohttp.ClientSession() as self.session:
            for i, url in enumerate(self.article_urls):
                count = visits_per_url + (1 if i < remainder else 0)
                for _ in range(count):
                    tasks.append(self.visit_url(url))
            
            await asyncio.gather(*tasks)
        
        logger.info(f"Normal visits completed: {self.success_count} success, {self.failure_count} failure")

    async def run_visits(self):
        """æ‰§è¡Œæ‰€æœ‰è®¿é—®ä»»åŠ¡"""
        start_time = datetime.now()
        
        # åŠ è½½å¿…åˆ·URL
        if not await self.load_required_urls():
            await self.send_notification("âš ï¸ åˆå§‹åŒ–å¤±è´¥: æ— æ³•åŠ è½½å¿…åˆ·URLåˆ—è¡¨")
            return
        
        # æ‰§è¡Œè®¿é—®
        await self.run_required_visits()
        await self.run_normal_visits()
        
        # ç”ŸæˆæŠ¥å‘Š
        end_time = datetime.now()
        await self.send_report(start_time, end_time)

    async def send_report(self, start_time: datetime, end_time: datetime):
        """ç”Ÿæˆå¹¶å‘é€ç»Ÿè®¡æŠ¥å‘Š"""
        duration = (end_time - start_time).total_seconds()
        total_requests = (
            self.required_stats['success'] + self.required_stats['failure'] +
            self.success_count + self.failure_count
        )
        rps = total_requests / duration if duration > 0 else 0
        
        # æ„å»ºæ¶ˆæ¯
        message = [
            "ğŸ“Š åšå®¢è®¿é—®ç»Ÿè®¡æŠ¥å‘Š",
            f"â±ï¸ æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%H:%M:%S')}",
            f"â³ è€—æ—¶: {duration:.1f}ç§’ | ğŸ“ˆ é€Ÿåº¦: {rps:.1f}æ¬¡/ç§’",
            "",
            "ğŸ”´ å¿…åˆ·URLç»Ÿè®¡:",
            f"  âœ… æˆåŠŸ: {self.required_stats['success']}",
            f"  âŒ å¤±è´¥: {self.required_stats['failure']}",
            "",
            "ğŸŸ¢ æ™®é€šè®¿é—®ç»Ÿè®¡:",
            f"  ğŸ¯ ç›®æ ‡: {self.total_visits}æ¬¡",
            f"  âœ… æˆåŠŸ: {self.success_count}",
            f"  âŒ å¤±è´¥: {self.failure_count}",
            f"  ğŸ“Š æˆåŠŸç‡: {self.success_count/self.total_visits*100:.1f}%" if self.total_visits > 0 else "",
            "",
            "ğŸ“Œ å¿…åˆ·URLè®¿é—®åˆ†å¸ƒ:"
        ]
        
        # æ·»åŠ URLè¯¦æƒ…
        for url, count in self.required_urls.items():
            message.append(f"  - {url.replace(BLOG_URL, '')}: {count}æ¬¡")
        
        message.extend([
            "",
            "ğŸ“ æ™®é€šURLè®¿é—®åˆ†å¸ƒ:"
        ])
        
        for url, count in self.visited_urls.items():
            message.append(f"  - {url.replace(BLOG_URL, '')}: {count}æ¬¡")
        
        message.extend([
            "",
            f"ğŸŒ åšå®¢åœ°å€: {BLOG_URL}"
        ])
        
        await self.send_notification('\n'.join(filter(None, message)))

    async def send_notification(self, message: str):
        """é€šè¿‡Telegramå‘é€é€šçŸ¥"""
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
        chat_id = os.getenv("TELEGRAM_CHAT_ID")
        
        if not bot_token or not chat_id:
            logger.warning("Telegramé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é€šçŸ¥")
            return
            
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown",
            "disable_web_page_preview": True
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=TELEGRAM_TIMEOUT)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        logger.info("Telegramé€šçŸ¥å‘é€æˆåŠŸ")
                    else:
                        error = await response.text()
                        logger.error(f"Telegramå‘é€å¤±è´¥: HTTP {response.status} - {error}")
        except Exception as e:
            logger.error(f"å‘é€Telegramé€šçŸ¥å‡ºé”™: {str(e)}")

def main():
    parser = argparse.ArgumentParser(description="Typechoåšå®¢è®¿é—®æ¨¡æ‹Ÿå™¨")
    parser.add_argument(
        "-n", "--visits",
        type=int,
        default=100,
        help="æ™®é€šè®¿é—®æ¬¡æ•°(å¿…åˆ·URLä¸è®¡å…¥æ­¤æ•°é‡)ï¼Œé»˜è®¤100æ¬¡"
    )
    args = parser.parse_args()
    
    visitor = BlogVisitor(args.visits)
    asyncio.run(visitor.run_visits())

if __name__ == "__main__":
    main()
