#!/usr/bin/env python3
"""
Typechoåšå®¢è‡ªåŠ¨è®¿é—®è„šæœ¬
åŠŸèƒ½ï¼š
1. ä»posts.txtåŠ è½½å¿…åˆ·URLåˆ—è¡¨
2. è‡ªåŠ¨å‘ç°åšå®¢æ–‡ç« 
3. å‡åŒ€åˆ†é…è®¿é—®é‡
4. Telegramé€šçŸ¥
5. è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
"""

import asyncio
import aiohttp
import random
import argparse
from datetime import datetime
import logging
import os
import sys
from typing import Dict, List, Tuple

# åŸºç¡€é…ç½®
CONFIG = {
    'blog_url': 'https://www.207725.xyz',
    'posts_file': 'posts.txt',
    'telegram_timeout': 10,
    'request_timeout': 15,
    'min_delay': 0.3,
    'max_delay': 2.0,
    'default_urls': [
        '/index.php/archives/13/',
        '/index.php/archives/5/'
    ]
}

# ç”¨æˆ·ä»£ç†æ±  (2024å¹´æœ€æ–°ç‰ˆ)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36 Edg/125.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148",
    "Mozilla/5.0 (Linux; Android 13; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0"
]

class BlogVisitor:
    def __init__(self, visits: int):
        self.normal_visits = visits
        self.stats = {
            'required': {'success': 0, 'failure': 0, 'urls': {}},
            'normal': {'success': 0, 'failure': 0, 'urls': {}}
        }
        self.start_time = datetime.now()
        self._setup_logging()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ | æ™®é€šè®¿é—®é‡: {visits}")
        logger.info(f"Python {sys.version.split()[0]} | {sys.platform}")

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        global logger
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('visit.log', encoding='utf-8')
            ]
        )
        logger = logging.getLogger('typecho_visitor')

    async def _get_urls(self) -> Tuple[List[str], List[str]]:
        """è·å–URLåˆ—è¡¨ï¼šè¿”å›(å¿…åˆ·URLs, æ™®é€šURLs)"""
        required_urls = await self._load_required_urls()
        normal_urls = await self._discover_urls()
        return required_urls, normal_urls or self._get_fallback_urls()

    async def _load_required_urls(self) -> List[str]:
        """åŠ è½½å¿…åˆ·URLåˆ—è¡¨"""
        try:
            if not os.path.exists(CONFIG['posts_file']):
                with open(CONFIG['posts_file'], 'w', encoding='utf-8') as f:
                    f.write('\n'.join(CONFIG['default_urls']))
                logger.warning(f"å·²åˆ›å»ºé»˜è®¤ {CONFIG['posts_file']}")
            
            with open(CONFIG['posts_file'], 'r', encoding='utf-8') as f:
                return [self._normalize_url(line.strip()) for line in f if line.strip()]
        except Exception as e:
            logger.error(f"åŠ è½½å¿…åˆ·URLå¤±è´¥: {str(e)}")
            return []

    async def _discover_urls(self) -> List[str]:
        """è‡ªåŠ¨å‘ç°æ–‡ç« URL"""
        try:
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            timeout = aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(CONFIG['blog_url'], headers=headers) as resp:
                    if resp.status == 200:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(await resp.text(), 'html.parser')
                        return list({
                            self._normalize_url(a['href'])
                            for a in soup.find_all('a', href=True)
                            if '/archives/' in a['href']
                        })
                    logger.warning(f"å‘ç°æ–‡ç« å¤±è´¥ HTTP {resp.status}")
        except Exception as e:
            logger.error(f"æ–‡ç« å‘ç°é”™è¯¯: {str(e)}")
        return []

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if url.startswith('http'):
            return url
        return f"{CONFIG['blog_url']}{url if url.startswith('/') else '/' + url}"

    def _get_fallback_urls(self) -> List[str]:
        """è·å–å¤‡ç”¨URLåˆ—è¡¨"""
        return [self._normalize_url(url) for url in CONFIG['default_urls']]

    async def _visit(self, session: aiohttp.ClientSession, url: str, is_required: bool):
        """æ‰§è¡Œå•æ¬¡è®¿é—®"""
        try:
            await asyncio.sleep(random.uniform(CONFIG['min_delay'], CONFIG['max_delay']))
            
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Referer': CONFIG['blog_url'],
                'Accept': 'text/html,application/xhtml+xml',
                'Accept-Language': 'en-US,en;q=0.9'
            }
            
            async with session.get(
                url,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=CONFIG['request_timeout'])
            ) as resp:
                key = 'required' if is_required else 'normal'
                if resp.status == 200:
                    self.stats[key]['success'] += 1
                    self.stats[key]['urls'][url] = self.stats[key]['urls'].get(url, 0) + 1
                    logger.debug(f"è®¿é—®æˆåŠŸ: {url}")
                else:
                    self.stats[key]['failure'] += 1
                    logger.warning(f"è®¿é—®å¤±è´¥: {url} (HTTP {resp.status})")
        except Exception as e:
            key = 'required' if is_required else 'normal'
            self.stats[key]['failure'] += 1
            logger.error(f"è®¿é—®é”™è¯¯: {url} - {str(e)}")

    async def _run_visits(self, urls: List[str], is_required: bool = False):
        """æ‰§è¡Œæ‰¹é‡è®¿é—®"""
        if not urls:
            return logger.error("æ— æœ‰æ•ˆURLå¯è®¿é—®")
            
        logger.info(f"å¼€å§‹{'å¿…åˆ·' if is_required else 'æ™®é€š'}è®¿é—® | URLæ•°é‡: {len(urls)}")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            if is_required:
                tasks = [self._visit(session, url, True) for url in urls]
            else:
                base_count = self.normal_visits // len(urls)
                extra = self.normal_visits % len(urls)
                for i, url in enumerate(urls):
                    count = base_count + (1 if i < extra else 0)
                    tasks.extend([self._visit(session, url, False) for _ in range(count)])
            
            await asyncio.gather(*tasks)

    async def _send_report(self):
        """å‘é€ç»Ÿè®¡æŠ¥å‘Š"""
        duration = (datetime.now() - self.start_time).total_seconds()
        total_success = sum(v['success'] for v in self.stats.values())
        total_failure = sum(v['failure'] for v in self.stats.values())
        
        def format_urls(url_type: str) -> str:
            return '\n'.join(
                f"  - {url.replace(CONFIG['blog_url'], '')}: {count}æ¬¡"
                for url, count in self.stats[url_type]['urls'].items()
            )
        
        message = [
            "âœ¨ Typechoè®¿é—®ç»Ÿè®¡æŠ¥å‘Š",
            f"â±ï¸ æ€»è€—æ—¶: {duration:.1f}ç§’",
            f"ğŸŒ åšå®¢åœ°å€: {CONFIG['blog_url']}",
            "",
            "ğŸ”´ å¿…åˆ·URLç»Ÿè®¡:",
            f"  âœ… æˆåŠŸ: {self.stats['required']['success']}æ¬¡",
            f"  âŒ å¤±è´¥: {self.stats['required']['failure']}æ¬¡",
            "",
            "ğŸŸ¢ æ™®é€šè®¿é—®ç»Ÿè®¡:",
            f"  ğŸ¯ ç›®æ ‡: {self.normal_visits}æ¬¡",
            f"  âœ… æˆåŠŸ: {self.stats['normal']['success']}æ¬¡",
            f"  âŒ å¤±è´¥: {self.stats['normal']['failure']}æ¬¡",
            f"  ğŸ“Š æˆåŠŸç‡: {self.stats['normal']['success']/self.normal_visits*100:.1f}%" if self.normal_visits > 0 else "",
            "",
            "ğŸ“Œ å¿…åˆ·URLè®¿é—®åˆ†å¸ƒ:",
            format_urls('required'),
            "",
            "ğŸ“ æ™®é€šURLè®¿é—®åˆ†å¸ƒ:",
            format_urls('normal')
        ]
        
        await self._notify_telegram('\n'.join(filter(None, message)))

    async def _notify_telegram(self, text: str):
        """å‘é€Telegramé€šçŸ¥"""
        if not (token := os.getenv('TELEGRAM_BOT_TOKEN')) or not (chat_id := os.getenv('TELEGRAM_CHAT_ID')):
            return logger.warning("æœªé…ç½®Telegramé€šçŸ¥")
            
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        payload = {
            'chat_id': chat_id,
            'text': text,
            'parse_mode': 'Markdown',
            'disable_web_page_preview': True
        }
        
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=CONFIG['telegram_timeout'])
            ) as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status != 200:
                        logger.error(f"Telegramé€šçŸ¥å¤±è´¥: HTTP {resp.status}")
        except Exception as e:
            logger.error(f"Telegramé€šçŸ¥é”™è¯¯: {str(e)}")

    async def execute(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        required_urls, normal_urls = await self._get_urls()
        
        await self._run_visits(required_urls, is_required=True)
        await self._run_visits(normal_urls)
        
        await self._send_report()
        logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

def main():
    parser = argparse.ArgumentParser(
        description="Typechoåšå®¢è®¿é—®æ¨¡æ‹Ÿå™¨",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '-n', '--visits',
        type=int,
        default=100,
        help='æ™®é€šè®¿é—®æ¬¡æ•°ï¼ˆå¿…åˆ·URLä¸è®¡å…¥ï¼‰'
    )
    args = parser.parse_args()
    
    visitor = BlogVisitor(args.visits)
    asyncio.run(visitor.execute())

if __name__ == '__main__':
    main()
