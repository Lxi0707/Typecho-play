#!/usr/bin/env python3
"""
Typechoåšå®¢æ™ºèƒ½è®¿é—®è„šæœ¬V3.0
åŠŸèƒ½ï¼š
1. å¿…åˆ·URLç‹¬ç«‹é…ç½®å’Œç‹¬ç«‹è®¿é—®æ¬¡æ•°æ§åˆ¶
2. æ™®é€šURLè‡ªåŠ¨å‘ç°å’Œè®¿é—®é‡åˆ†é…
3. æ™ºèƒ½é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. è¯¦ç»†çš„Telegramç»Ÿè®¡æŠ¥å‘Š
"""

import asyncio
import aiohttp
import random
import argparse
from datetime import datetime
import logging
import os
import sys
from typing import Dict, List, Tuple, Optional

# å…¨å±€é…ç½®
CONFIG = {
    'blog_url': 'https://www.207725.xyz',
    'posts_file': 'posts.txt',
    'telegram_timeout': 15,
    'min_timeout': 10,
    'max_timeout': 30,
    'min_delay': 0.8,
    'max_delay': 3.5,
    'max_retries': 2,
    'conn_limit': 15,
    'req_per_url': 3,  # æ¯ä¸ªå¿…åˆ·URLçš„è®¿é—®æ¬¡æ•°
    'default_urls': [
        '/index.php/archives/13/',
        '/index.php/archives/5/'
    ]
}

# ç°ä»£æµè§ˆå™¨ç”¨æˆ·ä»£ç†
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 13; SM-S901B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Mobile Safari/537.36"
]

class TypechoVisitorPro:
    def __init__(self, normal_visits: int, required_visits: Optional[int] = None):
        self.normal_visits = normal_visits
        self.required_visits_per_url = required_visits or CONFIG['req_per_url']
        self.stats = {
            'required': {'success': 0, 'failure': 0, 'urls': {}},
            'normal': {'success': 0, 'failure': 0, 'urls': {}}
        }
        self.session = None
        self.start_time = datetime.now()
        self._setup_logging()
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ | æ™®é€šè®¿é—®: {normal_visits}æ¬¡ | å¿…åˆ·è®¿é—®: {self.required_visits_per_url}æ¬¡/URL")

    def _setup_logging(self):
        """é…ç½®å¤šå±‚çº§æ—¥å¿—ç³»ç»Ÿ"""
        global logger
        logger = logging.getLogger('typecho_pro')
        logger.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # æ§åˆ¶å°è¾“å‡º
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # æ–‡ä»¶è¾“å‡º
        file_handler = logging.FileHandler('visit_pro.log', encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

    async def _get_connector(self):
        """åˆ›å»ºä¼˜åŒ–åçš„è¿æ¥å™¨"""
        return aiohttp.TCPConnector(
            limit=CONFIG['conn_limit'],
            force_close=False,
            enable_cleanup_closed=True,
            ssl=False
        )

    async def _get_url_lists(self) -> Tuple[List[str], List[str]]:
        """è·å–URLåˆ†ç±»åˆ—è¡¨ï¼š(å¿…åˆ·URLs, æ™®é€šURLs)"""
        required_urls = await self._load_required_urls()
        normal_urls = await self._discover_normal_urls()
        return required_urls, normal_urls or self._get_fallback_urls()

    async def _load_required_urls(self) -> List[str]:
        """åŠ è½½å¿…åˆ·URLåˆ—è¡¨"""
        try:
            if not os.path.exists(CONFIG['posts_file']):
                with open(CONFIG['posts_file'], 'w', encoding='utf-8') as f:
                    f.write('\n'.join(CONFIG['default_urls']))
                logger.warning(f"å·²åˆ›å»ºé»˜è®¤ {CONFIG['posts_file']}")
            
            with open(CONFIG['posts_file'], 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
                return [self._normalize_url(url) for url in urls]
        except Exception as e:
            logger.error(f"åŠ è½½å¿…åˆ·URLå¤±è´¥: {str(e)}")
            return []

    async def _discover_normal_urls(self) -> List[str]:
        """å‘ç°æ™®é€šæ–‡ç« URL"""
        try:
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            timeout = aiohttp.ClientTimeout(total=CONFIG['max_timeout'])
            
            async with aiohttp.ClientSession(
                connector=await self._get_connector(),
                timeout=timeout
            ) as session:
                async with session.get(CONFIG['blog_url'], headers=headers) as resp:
                    if resp.status == 200:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(await resp.text(), 'html.parser')
                        
                        # å¤šæ¡ä»¶ç­›é€‰æœ‰æ•ˆæ–‡ç« é“¾æ¥
                        articles = set()
                        for a in soup.find_all('a', href=True):
                            href = a['href']
                            if '/archives/' in href and not any(x in href for x in ['#', 'feed', 'xmlrpc', 'comment']):
                                articles.add(self._normalize_url(href))
                        return list(articles)
                    
                    logger.warning(f"å‘ç°æ–‡ç« å¤±è´¥ HTTP {resp.status}")
        except Exception as e:
            logger.error(f"æ–‡ç« å‘ç°å¼‚å¸¸: {str(e)}")
        return []

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if url.startswith('http'):
            return url.split('#')[0]
        return f"{CONFIG['blog_url']}{url if url.startswith('/') else '/' + url}".split('#')[0]

    def _get_fallback_urls(self) -> List[str]:
        """è·å–å¤‡ç”¨URLåˆ—è¡¨"""
        return [self._normalize_url(url) for url in CONFIG['default_urls']]

    async def _visit_url(self, session: aiohttp.ClientSession, url: str, is_required: bool):
        """æ‰§è¡Œå•æ¬¡URLè®¿é—®ï¼ˆå«æ™ºèƒ½é‡è¯•ï¼‰"""
        key = 'required' if is_required else 'normal'
        timeout = random.randint(CONFIG['min_timeout'], CONFIG['max_timeout'])
        
        for attempt in range(CONFIG['max_retries'] + 1):
            try:
                await asyncio.sleep(random.uniform(CONFIG['min_delay'], CONFIG['max_delay']))
                
                headers = {
                    'User-Agent': random.choice(USER_AGENTS),
                    'Accept': 'text/html,application/xhtml+xml',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Referer': CONFIG['blog_url']
                }
                
                async with session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=timeout)
                ) as resp:
                    if resp.status == 200:
                        self.stats[key]['success'] += 1
                        self.stats[key]['urls'][url] = self.stats[key]['urls'].get(url, 0) + 1
                        logger.debug(f"è®¿é—®æˆåŠŸ: {url}")
                        return True
                    logger.warning(f"è®¿é—®å¤±è´¥: {url} (HTTP {resp.status})")
            except Exception as e:
                logger.warning(f"è®¿é—®å¼‚å¸¸: {url} - {type(e).__name__}")
            
            if attempt < CONFIG['max_retries']:
                await asyncio.sleep(1.5)  # é‡è¯•é—´éš”
        
        self.stats[key]['failure'] += 1
        return False

    async def _run_required_visits(self, urls: List[str]):
        """æ‰§è¡Œå¿…åˆ·URLè®¿é—®"""
        if not urls:
            logger.warning("æ²¡æœ‰é…ç½®å¿…åˆ·URLï¼Œè·³è¿‡è¯¥é˜¶æ®µ")
            return
            
        logger.info(f"å¼€å§‹å¿…åˆ·è®¿é—® | URLæ•°é‡: {len(urls)} | æ¯URLæ¬¡æ•°: {self.required_visits_per_url}")
        
        async with aiohttp.ClientSession(
            connector=await self._get_connector(),
            timeout=aiohttp.ClientTimeout(total=CONFIG['max_timeout'])
        ) as session:
            tasks = []
            for url in urls:
                for _ in range(self.required_visits_per_url):
                    tasks.append(self._visit_url(session, url, True))
            
            await asyncio.gather(*tasks)
        
        logger.info(f"å¿…åˆ·è®¿é—®å®Œæˆ | æˆåŠŸ: {self.stats['required']['success']} | å¤±è´¥: {self.stats['required']['failure']}")

    async def _run_normal_visits(self, urls: List[str]):
        """æ‰§è¡Œæ™®é€šURLè®¿é—®"""
        if self.normal_visits <= 0:
            logger.info("æ™®é€šè®¿é—®æ¬¡æ•°ä¸º0ï¼Œè·³è¿‡è¯¥é˜¶æ®µ")
            return
            
        if not urls:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ™®é€šURLï¼Œè·³è¿‡è¯¥é˜¶æ®µ")
            return
            
        logger.info(f"å¼€å§‹æ™®é€šè®¿é—® | URLæ•°é‡: {len(urls)} | æ€»æ¬¡æ•°: {self.normal_visits}")
        
        # è®¡ç®—åˆ†é…é‡
        base_visits = self.normal_visits // len(urls)
        extra_visits = self.normal_visits % len(urls)
        
        async with aiohttp.ClientSession(
            connector=await self._get_connector(),
            timeout=aiohttp.ClientTimeout(total=CONFIG['max_timeout'])
        ) as session:
            tasks = []
            for i, url in enumerate(urls):
                visits = base_visits + (1 if i < extra_visits else 0)
                for _ in range(visits):
                    tasks.append(self._visit_url(session, url, False))
            
            await asyncio.gather(*tasks)
        
        logger.info(f"æ™®é€šè®¿é—®å®Œæˆ | æˆåŠŸ: {self.stats['normal']['success']} | å¤±è´¥: {self.stats['normal']['failure']}")

    def _generate_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š"""
        duration = (datetime.now() - self.start_time).total_seconds()
        req = self.stats['required']
        norm = self.stats['normal']
        total_success = req['success'] + norm['success']
        total_failure = req['failure'] + norm['failure']
        
        def format_urls(urls: Dict[str, int], limit: int = 15) -> str:
            sorted_items = sorted(urls.items(), key=lambda x: -x[1])
            return '\n'.join(
                f"  - {url.replace(CONFIG['blog_url'], ''):<35}: {count}æ¬¡"
                for url, count in sorted_items[:limit]
            )
        
        return (
            "ğŸ“Š Typechoä¸“ä¸šè®¿é—®æŠ¥å‘Š\n\n"
            f"â±ï¸ æ€»è€—æ—¶: {duration:.1f}ç§’ | ğŸŒ åšå®¢: {CONFIG['blog_url']}\n\n"
            "ğŸ”´ å¿…åˆ·è®¿é—®ç»Ÿè®¡:\n"
            f"  â€¢ ç›®æ ‡: {len(req['urls'])} URLs Ã— {self.required_visits_per_url}æ¬¡\n"
            f"  â€¢ æˆåŠŸ: {req['success']}æ¬¡\n"
            f"  â€¢ å¤±è´¥: {req['failure']}æ¬¡\n"
            f"  â€¢ æˆåŠŸç‡: {req['success']/(req['success']+req['failure'])*100:.1f}%\n\n"
            "ğŸŸ¢ æ™®é€šè®¿é—®ç»Ÿè®¡:\n"
            f"  â€¢ ç›®æ ‡: {self.normal_visits}æ¬¡\n"
            f"  â€¢ æˆåŠŸ: {norm['success']}æ¬¡\n"
            f"  â€¢ å¤±è´¥: {norm['failure']}æ¬¡\n"
            f"  â€¢ æˆåŠŸç‡: {norm['success']/self.normal_visits*100:.1f}%\n\n"
            "ğŸ“Œ å¿…åˆ·URLè®¿é—®è¯¦æƒ…:\n"
            f"{format_urls(req['urls'])}\n\n"
            "ğŸ“ æ™®é€šURLè®¿é—®TOP15:\n"
            f"{format_urls(norm['urls'])}\n\n"
            "ğŸ’¯ ç»¼åˆç»Ÿè®¡:\n"
            f"  â€¢ æ€»æˆåŠŸ: {total_success}æ¬¡\n"
            f"  â€¢ æ€»å¤±è´¥: {total_failure}æ¬¡\n"
            f"  â€¢ æ€»æˆåŠŸç‡: {total_success/(total_success+total_failure)*100:.1f}%"
        )

    async def _send_report(self):
        """å‘é€TelegramæŠ¥å‘Š"""
        report = self._generate_report()
        logger.info("\n" + report)
        
        if not (token := os.getenv('TELEGRAM_BOT_TOKEN')) or not (chat_id := os.getenv('TELEGRAM_CHAT_ID')):
            return logger.warning("æœªé…ç½®Telegramé€šçŸ¥ï¼Œè·³è¿‡å‘é€")
            
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        payload = {
            'chat_id': chat_id,
            'text': report,
            'parse_mode': 'Markdown',
            'disable_web_page_preview': True
        }
        
        try:
            async with aiohttp.ClientSession(
                connector=await self._get_connector(),
                timeout=aiohttp.ClientTimeout(total=CONFIG['telegram_timeout'])
            ) as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status != 200:
                        logger.error(f"æŠ¥å‘Šå‘é€å¤±è´¥ HTTP {resp.status}")
        except Exception as e:
            logger.error(f"å‘é€æŠ¥å‘Šå¼‚å¸¸: {str(e)}")

    async def execute(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        required_urls, normal_urls = await self._get_url_lists()
        
        await self._run_required_visits(required_urls)
        await self._run_normal_visits(normal_urls)
        
        await self._send_report()
        logger.info("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•")

def main():
    parser = argparse.ArgumentParser(
        description="Typechoä¸“ä¸šè®¿é—®è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument(
        '-n', '--normal-visits',
        type=int,
        default=500,
        help='æ™®é€šè®¿é—®ç›®æ ‡æ¬¡æ•°'
    )
    parser.add_argument(
        '-r', '--required-visits',
        type=int,
        help='æ¯ä¸ªå¿…åˆ·URLçš„è®¿é—®æ¬¡æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰'
    )
    args = parser.parse_args()
    
    visitor = TypechoVisitorPro(
        normal_visits=args.normal_visits,
        required_visits=args.required_visits
    )
    asyncio.run(visitor.execute())

if __name__ == '__main__':
    main()
